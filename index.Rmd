---
title: "Financial Inclusion in Africa"
author: "Molo Muli"
date: "4/8/2021"
output: 
  html_document:
  css: style.css
  number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r R-Scripts, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Libraries
library(easypackages)
libraries("dlookr","knitr","kableExtra")

## Script Files
source("Scripts/FIIA.DataPreparation.R", local = knitr::knit_global())
sys.source("Scripts/Analysis.R", envir = knitr::knit_global())
sys.source("Scripts/MachineLearning.R", envir = knitr::knit_global())
```
# Research question and the data science problem
* The research question is anchored on the main objective of this project which is create a machine learning model that can predict which individuals are most likely to have or use a bank account. 
* The data science problem will be a classification problem.
* My personal objective on this project is how to handle an imbalanced dataset in a classification problem


# Data Health and Preparation

Training data contains 23,524 observations and 13 variables. From the total variables, 13 are nominal (including the response column) and 3 numeric. The independent variable is **bank_account** which has observations as yes or no. Part of data wrangling will entail encoding observations of the response variable to match 1 and 0 for yes and no respectively. A quick glimpse on the response column observations indicate a high imbalance with 86 percents of all persons not having a bank account.

No missing values are present in the dataset.

Half of the all respondents are of 35 years of age whereas 75% of all respondents are 49 years and below. Householdwise, the largest household consisted of 21 members given the average number being 4. On both variables, measures of spread indicated presence of outliers. Using Tukey's rule of outlier detection, 2.77% of all observations had outliers. Given their marginal proportion, the outliers were dropped.

# Data Analysis

## Descriptive
On both the variables, the distribution is light tailed (platykurtic). Outliers, which were marginal in relative to the total sample were removed during the data wrangling process. 
Age has a high standard deviation indicating that values are spread out around the mean. Standard deviation (σ) as a measure of dispersion, its an indicator of how accurately the mean(μ) represents the sample data.

```{r Numeric Descriptives, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
kbl(numeric.descriptives.training.set, booktabs = T) %>% 
  # kable_styling(latex_options = "striped") %>%
  kable_classic(full_width = F, html_font = "Cambria", position = "float_right") %>%
  footnote(number = "**table 1 : Descritpive statistitcs of age and the household size**")
```
Standard Error of the mean (SEM) on the other hand, measures how far the mean of the sample is likely to be from the true population mean. SEM is be inversely proportional to the sample size. As the sample size increases the mean value becomes more representative of the population, hence SEM reduces towards zero.

From *table one*, we can see that the SEM is more close to zero indicating that the mean of the sample close to the mean of the total sample. 
In deducing any of the coefficients mentioned above, primary assumption is that all observations from the sample are statistically independent.  

```{r Normality Test, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.show="hold", out.width="85%"}
par(mfrow=c(2,2))
qqnorm(training.set$Age, main = "Normal Q-Q Plot for Age") 
qqnorm(training.set$`Household Size`, main = "Normal Q-Q Plot for Household Size") 
# qqline(training.set$Age, col = "red")
# qqline(training.set$`Household Size`, col = "red")
```

## Exploratory
<div class = "row">
<hr>

<div  class = "col-md-6">
```{r Mean Age of who own a bank account, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=6}
MeanAgeBankAccountPlot
```
</div>
  
<div class = "col-md-6">
```{r Bank account holders per gender, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=6}
GenderBA
````
</div>
* Fig I: Mean age of all individuals who both have and don't have a bank account is 38 and 39 respectively.

* Fig II: Almost 90% of all females do not have a bank account
<hr>
</div>

<!--Relationship  of the individual to the Head of household-->

<!--Third chart takes the full-->
<div class = "row">
<div class = "col-md-12">
```{r Relationship of the respondent to the HH Head-BA, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=12, fig.height=6}
GenderHHBA
````
</div>
* 72% of all male headed household possess a bank account
* 9 in 10 females own a bank account where a female is a spouse.
<hr>
</div>


<!--Fourth chart takes the full-->
<div class = "row">
<div class = "col-md-12">
```{r Relationship of the respondent to the HH Head-NoBA, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=12, fig.height=6}
GenderHHNoBA
````
</div>
* 57 % of male headed households do not have a bank account as compared to female counterparts (43%)
<hr>
</div>

<!--Education levels of the individuals with respect to the   of the individual to the Head of household-->

<!--Fifth Chart-->
<div class = "row">
<div class = "col-md-12">
```{r Education Levels -BA, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=12, fig.height=6}
EducationGenderBA
````
</div>
* 56% of females who own a bank account have no formal education as compared to their male counterparts (43.8%).
<hr>
</div>

<!--Sixth Chart-->
<div class = "row">
<div class = "col-md-12">
```{r Education Levels-NoBA, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=12, fig.height=6}
EducationGenderNoBA
````
</div>
* 71% of of female respondents who don't own a bank account, don't have a formal education. This is in contrast with 29% of male respondents
* Finally, regardless of presence or absence of a bank account female respondents account the highest demographic of not undergone formal education.
<hr>
</div>


# Machine Learning

## Random Forests
### Random Forest Classifier

```{r Random Forest Classifier, echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
# The classifier
model <- randomForest(formula = BankAccount ~ ., data = training)
model
```
From the model;

* Variables randomly selected at each split are 3.
* Out of the Bag error - a validation technique used in this model is 11.42%, hence the model has accuracy is 89%. 
* Finally, below is a plot showing variable importance. Job type is the most important explanatpory variable that determines if an individual will open a bank account or not. Its closely followed by education level of a person and age.


```{r Variable importance, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=10, fig.height=5}
varImportancePlot
```

### Tuning the classifier
Hyperparameter tuning is usually treated as an optimization problem. Even though random forest is an out of the box algorithm, there are two important parameters that greatly affect the accuracy of the model and hence need tuning. These are `ntrees` and `mtry`. `ntrees` is the number of trees whereas `mtry` denotes the number of random variables selected on each split. 



